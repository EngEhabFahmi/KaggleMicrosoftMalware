__author__ = 'David Reilly'

from itertools import groupby
from pyspark import SparkContext, SparkConf
from pyspark.mllib.regression import LabeledPoint
from pyspark.mllib.classification import LogisticRegressionWithSGD
from pyspark.mllib.tree import DecisionTree
import urllib2
from nltk import FreqDist
from nltk.util import ngrams
import csv

"""
    This code was developed for accessing and parsing the bytes files from the
    Microsoft Malware Classification Challenge. For both training and testing data,
    the frequencies of each hex symbol are calculated. The end result is stored as
    a list of dictionaries.
"""

## This function returns an RDD where each row is an individual file
def path_to_whole_files(path):

    return sc.wholeTextFiles(path, 1000)

## This function returns an RDD from a path to a text file
def path_to_text_sc(path):

    return sc.textFile(path)

## This function adds the total length of a file
def get_denominators(rdd):

    return rdd.map(lambda x: (x[0], len(x[1])))

## This function performs the equivalent of Spark's reduceByKey to an element of a tuple
def sum_counts(list):

    return [(key, sum(j for i, j in group)) for key, group in groupby(sorted(list, key=lambda y: y[0]), key=lambda x: x[0][:3])]

## This function divides each count by the total number of counts
def divide_counts(list, denominator):

    return map(lambda x: (x[0], x[1] / float(denominator)), list)

## Remove ngrams that only appear once
def filter_grams(key, value, total):

    if value > 1:
        return key, (value / total)

## This function calculates n-gram frequencies
def add_ngram_frequencies(list_of_text, n):

    ngram_frequencies = FreqDist(ngrams(list_of_text, n))
    total = float(ngram_frequencies.N())

    frequencies = dict(filter(lambda x: x is not None,
                              map(lambda (key, value): filter_grams(key, value, total),
                                  bigram_fd.iteritems())))

    return frequencies

## This function removes unnecessary information from a file name
def strip_file_names(string_of_name):

    splits = string_of_name.split(".")
    name = splits[0][-20:]

    return name

## This function maps strip_file_names to each row of an RDD, cleaning the name of the file
def strip_names(rdd):

    return rdd.map(lambda x: (strip_file_names(x[0]), x[1]))

## This function transforms each row of an RDD with training data and labels into a dictionary
def create_file_dict(tuples):

    dictionary = dict(tuples[1])
    dictionary.update(tuples[4])
    dictionary.update(tuples[5])
    dictionary['class'] = tuples[0]
    dictionary['length'] = tuples[2]
    dictionary['file'] = tuples[3]

    return dictionary

## This function transforms each row of an RDD with test data into a dictionary
def create_final_test_dict(tuples):

    dictionary = dict(tuples[1])
    dictionary.update(tuples[3])
    dictionary.update(tuples[4])
    dictionary['length'] = tuples[2]
    dictionary['file'] = tuples[0]

    return dictionary

def bayes_counts(rdd, dict_of_classes, aggregate_by_class=True, return_counts=True):

    """
        This function is the heavy lifter of this script. It is responsible for performing
        each sequential map job to take a raw RDD into the final product.

        Inputs: an RDD, a dictionary mapping file names to malware classification
        Outputs: an RDD
        This function takes in has two purposes.
        1) When aggregate_by_class=True:
            This function aggregates files together by class. It then counts
            hex symbol frequencies for the purposes of naive bayes analysis.
        2) When aggregate_by_class=False:
            This function does not aggregate by class, and, instead, returns
            either counts (return_counts=True) or proportions (return_counts=False)
            of hex symbols for each file.
    """

    stripped_names = strip_names(rdd)
    classes = get_classes(dict_of_classes, stripped_names)

    if aggregate_by_class:
        next_rdd = stripped_names.reduceByKey(lambda a, b: a + b)

    else:
        next_rdd = classes

    # Split each line in each file
    individual_pairs = next_rdd.map(lambda y: (y[0], y[1].split(), y[2]))

    ## Remove line numbers
    removed_line_numbers = individual_pairs.map(lambda x: (x[0], filter(lambda y: len(y) < 3, x[1]), x[2]))

    ## Add bigram and quadgram frequencies
    bigrams = removed_line_numbers.map(lambda x: (x[0], x[1], x[2], add_ngram_frequencies(x[1], 2)))
    quadgrams = bigrams.map(lambda x: (x[0], x[1], x[2], x[3], add_ngram_frequencies(x[1], 4)))

    ## Add count of 1 to each element
    get_counts = quadgrams.map(lambda x: (x[0], map(lambda s: (s, 1), x[1]), len(x[1]), x[2], x[3], x[4]))

    ## Add together counts
    counted = get_counts.map(lambda x: (x[0], sum_counts(x[1]), x[2], x[3], x[4], x[5]))

    if return_counts:
        return counted

    ## Divide by total number of symbols
    proportions = counted.map(lambda x: (x[0], divide_counts(x[1], x[2]), x[2], x[3], x[4], x[5]))

    ## Create dictionary
    final = proportions.map(lambda x: create_file_dict(x))

    return final

def bayes_counts_test(rdd, aggregate_by_class=True, return_counts=True):

    """
        Inputs: an RDD, a dictionary mapping file names to malware classification
        Outputs: an RDD
        This function takes in has two purposes.
        1) When aggregate_by_class=True:
            This function aggregates files together by class. It then counts
            hex symbol frequencies for the purposes of naive bayes analysis.
        2) When aggregate_by_class=False:
            This function does not aggregate by class, and, instead, returns
            either counts (return_counts=True) or proportions (return_counts=False)
            of hex symbols for each file.
    """

    stripped_names = strip_names(rdd)

    if aggregate_by_class:
        next_rdd = stripped_names.reduceByKey(lambda a, b: a + b)

    else:
        next_rdd = stripped_names

    # Split each line in each file
    individual_pairs = next_rdd.map(lambda y: (y[0], y[1].split()))

    ## Remove line numbers
    removed_line_numbers = individual_pairs.map(lambda x: (x[0], filter(lambda y: len(y) < 3, x[1])))

    ## Add bigrams and quadgrams
    bigrams = removed_line_numbers.map(lambda x: (x[0], x[1], add_ngram_frequencies(x[1], 2)))
    quadgrams = bigrams.map(lambda x: (x[0], x[1], x[2], add_ngram_frequencies(x[1], 4)))

    ## Add count of 1 to each element
    get_counts = quadgrams.map(lambda x: (x[0], map(lambda s: (s, 1), x[1]), len(x[1]), x[2], x[3]))

    ## Add together counts
    counted = get_counts.map(lambda x: (x[0], sum_counts(x[1]), x[2], x[3], x[4]))

    if return_counts:
        return counted

    ## Divide by total number of symbols
    proportions = counted.map(lambda x: (x[0], divide_counts(x[1], x[2]), x[2], x[3], x[4]))

    final = proportions.map(lambda x: create_final_test_dict(x))

    return final

## This function adds the observation's class to each row
def get_classes(dict_of_classes, rdd):

    return rdd.map(lambda x: (dict_of_classes[x[0]], x[1], x[0]))

## This function aggregates the observations by class
def aggregate_by_classes(rdd):

    return rdd.reduceByKey(lambda x, y: x + y)

## This function turns each row into a space-delimited string of values
def parseRdd(file):

    label = file[0]
    hex_values = [str(x[1]) for x in file[1]]
    file_length = str(file[2])
    hex_values.append(file_length)

    return str(label) + " " + " ".join(hex_values)

## This function turns each row into a Spark LabeledPoint data structure
def parsePoint(line):

    values = [float(x) for x in line.split(" ")]

    label = values[0]
    return LabeledPoint(label, values[1:])

## This function calls parsePoint on an RDD of space-delimited rows
def parseText(path_to_text):

    data = sc.textFile(path_to_text)

    parsed = data.map(parsePoint)
    return parsed

## This function returns a dictionary mapping file names to its class
def create_classes_dict(path_to_labels):

    response = urllib2.urlopen(path_to_labels)
    html = response.read()
    lines = html.split("\n")
    lines.pop(0)
    lines.pop(-1)

    classes = {}
    for el in lines:
        splits = el.split(",")
        classes[splits[0].strip('\"')] = int(splits[1])

    return classes

def main(train=True, test=False, path_to_train=None, path_to_labels=None,
         path_to_test=None, path_to_save_train=None, path_to_save_test=None):

    if train:
        rdd = path_to_whole_files(path_to_train)
        broadcastedVar = sc.broadcast(create_classes_dict(path_to_labels))
        proportions = bayes_counts(rdd, broadcastedVar.value, aggregate_by_class=False, return_counts=False)
        proportions.saveAsTextFile(path_to_save)

    if test:
        rdd = path_to_whole_files(path_to_test)
        proportions = bayes_counts_test(rdd, aggregate_by_class=False, return_counts=False)
        proportions.saveAsTextFile(path_to_save)


if __name__ == "__main__":

    path_to_bytes_file = "/path/to/s3/bytes/files"
    path_to_labels = "/path/to/train/labels"
    path_to_test = "/path/to/s3/bytes/test/files"

    conf = SparkConf().setAppName("MyApp")
    conf.set("spark.executor.memory", "6g")
    sc = SparkContext(conf=conf)

    main(train=True,
         test=True,
         path_to_train=path_to_bytes_file,
         path_to_labels=path_to_labels,
         path_to_test=path_to_test,
         path_to_save_train="/path/to/s3/bucket/train",
         path_to_save_test="/path/to/s3/bucket/test"
    )