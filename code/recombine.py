import re
import random
import pandas as pd
import re
#from pyspark import SparkContext
from boto.s3.connection import S3Connection
import glob 

def getFolderNames(stringPrefix, bucketName='malwaredata', 
	keyID='AKIAIAYSOVUQ37QIP4OA', secretKey='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'):
	conn = S3Connection(keyID, secretKey)
	bucket = conn.get_bucket(bucketName)
	ACAKeys = bucket.list(prefix=stringPrefix)
	bucketNames = []
	for k in ACAKeys: 
		bucketNames.append(re.match('(' + stringPrefix + '.*)\/.*', k.name).groups(0))
	bucketSet = list(set(bucketNames))
	return bucketSet

if __name__ == "__main__":
	#sc = SparkContext()
	folderNames = getFolderNames("Aggregated/ClassAgg")
    #print getBucketNames("ClassAgg")
	s3n1 = "s3n://AKIAJBLPYYHFYB7GJ33A:qxl+33luWGIgs50ASHNBxFKBUPEr1L2foz1dgljG@malwaredata/" + folderNames[0][0]

	#baseRDD = sc.textFile(s3n1, ).collect()


    #for folder in folderNames 
    #directories = list(malwarebucket.list("", "/"))
    #print directories 

