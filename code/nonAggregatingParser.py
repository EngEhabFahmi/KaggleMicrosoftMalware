#!/usr/bin/env python
# -*- coding: utf-8 -*-
import re
import random
import pandas as pd
from pyspark import SparkContext
import string

def asmReduce(a, b): 
    #(frequency_counter, section_length, num_subroutines, totalNumTokens)
    totalFreqDict = {}
    aKeys = set(a[0].keys())
    bKeys =set(b[0].keys())
    intersectKeys = aKeys.intersection(bKeys)
    onlyA = aKeys.difference(intersectKeys)
    onlyB = bKeys.difference(intersectKeys)
    for key in intersectKeys: 
       totalFreqDict[key] = a[0][key] + b[0][key]
    for key in onlyA: 
       totalFreqDict[key] = a[0][key]
    for key in onlyB: 
       totalFreqDict[key] = b[0][key]
    meanDict = {}
    aKeys2 = set(a[1].keys())
    bKeys2 = set(b[1].keys()) 
    intersectKeys2 = aKeys2.intersection(bKeys2) 
    onlyA2 = aKeys2.difference(intersectKeys2) 
    onlyB2 = bKeys2.difference(intersectKeys2)
    for key in intersectKeys2: 
        aVal = a[1][key]
        bVal = b[1][key]
        meanDict[key] = (aVal+bVal)/2.0
    for key in onlyA2: 
        meanDict[key] = a[1][key]
    for key in onlyB2: 
        meanDict[key] = b[1][key]
    avgNumSubroutines = (a[2]+b[2])/2.0
    totalNumTokens = a[3] + b[3]
    return totalFreqDict, meanDict, avgNumSubroutines, totalNumTokens


def parseFile(fileName, prefixes=False):
    cnt = {}
    sectionsSeen = {}
    numSubroutines = 0
    numTokens = 0
    f = fileName.split('\n')
    for line in f:
        if ".data:00" in line:
            startTrigger = "_data"
            if prefixes:
                prefix = "DT_"
            else:
                prefix = ""
            if "_data" not in sectionsSeen:
                triggered = False

        elif ".rdata:00" in line:
            startTrigger = "_rdata"
            if prefixes:
                prefix = "RDT_"
            else:
                prefix = ""

            if "_rdata" not in sectionsSeen:
                triggered = False

        elif ".idata:00" in line:
            if prefixes:
                prefix = "IDT_"
            else:
                prefix = ""
            startTrigger = "_idata"
            if "_idata" not in sectionsSeen:
                triggered = False
        else:
            startTrigger = "_text"
            prefix = ""
            if "_text" not in sectionsSeen:
                triggered = False

        if startTrigger in line:
            triggered = True
            if startTrigger not in sectionsSeen:
                sectionsSeen[startTrigger] = 0

        if triggered:
            sectionsSeen[startTrigger] += 1
            if "S U B R O U T I N E" in line:
                numSubroutines += 1
                continue
            else:
                lineSplit = line.split('       ')
                if len(lineSplit) > 1:
                    assembCode = lineSplit[1]
                else:
                    assembCode = lineSplit[0]
                tokens = assembCode.split()
                for token in tokens:
                    if hasNoStopwords(token):
                        token = re.sub(r'[,;]','',token)
                        numTokens += 1 
                        if 'rdata' in token:
                            token = prefix + 'rdata'
                        elif 'loc' in token:
                            token = prefix + 'loc'
                        else:
                            token = prefix + token
                        if token not in cnt: 
                            cnt[token] = 0
                        cnt[token] += 1 

    return cnt, sectionsSeen, numSubroutines, numTokens

def hasNoStopwords(token):
    stopwords = [".text:", '_text', 'Ãƒ', 'idata:', '_idata', 'rdata:', '_rdata', 'data:', '_data']
    for stopword in stopwords:
        if stopword in token:
            return False
    return True
def fileID(fileStr):
    isMatch = re.match('.*\/([0-9a-zA-Z]*)\.(asm|bytes)', fileStr)
    if isMatch:
        return isMatch.groups()[0]
    else:
        return ''

if __name__ == "__main__":
    sc = SparkContext()
    #for l in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E',
    #           'F', 'G', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'H', 'I', 'J', 'K', 'L']:
    for l in ['I', 'J', 'K', 'k', 'L', 'l', 'M', 'm', 'N', 'n', 'O', 'o', 'P', 'p', 'Q',
              'q', 'R', 'r', 'S', 's', 'T', 't', 'U', 'u', 'V', 'v', 'W', 'w', 'X', 'x', 'Y', 'y', 'Z', 'z']:
    #trainLabels = pd.read_csv("https://s3-us-west-2.amazonaws.com/malwaredata/trainLabels.csv")
    #trainDict = trainLabels.set_index('Id').to_dict()['Class']
    #labelsBC = sc.broadcast(trainDict)
    #asmFiles = sc.wholeTextFiles("s3n://malwaredata/train/" + l + "*.asm", 100).map(lambda x: (labelsBC.value[fileID(x[0])], parseFile(x[1].encode('utf-8'))))
        num = random.random()*100 
        saveFile  = "s3n://AKIAJBLPYYHFYB7GJ33A:qxl+33luWGIgs50ASHNBxFKBUPEr1L2foz1dgljG@malwaredata/TestUnAggregatedNoPre/unAgg_" + l + str(num)
        #saveFile  = "s3n://malwaredata/TestUnAggregatedPre/unAgg_" + l + str(num)
        #sc.wholeTextFiles("s3n://malwaredata/train/" + l + "*.asm", 100).map(lambda x: (labelsBC.value[fileID(x[0])], parseFile(x[1].encode('utf-8')))).reduceByKey(asmReduce).saveAsPickleFile(saveFile)
        #stores files names 

        #parse with prefixes = True
        #sc.wholeTextFiles("s3n://AKIAJ3INZECKQLA6A5LQ:tMIqQJvGMJwJDnASoxT+TFo5Jjb2VpBaVyzPFxdh@malwaretestdata/test/" + l + "*.asm", 100).map(lambda x: (fileID(x[0]), parseFile(x[1].encode('utf-8'), prefixes=True))).reduceByKey(asmReduce).saveAsTextFile(saveFile)
        #parse with prefixes = False
        sc.wholeTextFiles("s3n://AKIAJ3INZECKQLA6A5LQ:tMIqQJvGMJwJDnASoxT+TFo5Jjb2VpBaVyzPFxdh@malwaretestdata/test/" + l + "*.asm", 100).map(lambda x: (fileID(x[0]), parseFile(x[1].encode('utf-8'), prefixes=False))).reduceByKey(asmReduce).saveAsTextFile(saveFile)

        #asmFiles.saveAsTextFile(saveFile)
           
            

