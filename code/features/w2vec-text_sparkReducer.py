from gensim.models.word2vec import Word2Vec
from pyspark import SparkContext
import re
import math
import random
import numpy as np


def sentenceParse(fileStr, section='text', type="tup"):
    if section == 'text':
        startTrigger = "_text"
    triggered = False
    sentenceTokens = []
    listOfSentences = []
    i = 0
    if type == "tup":
        fileList = fileStr[1].split('\n')
    elif type == "list":
        fileList = fileStr
    elif type == "string":
        fileList = fileStr.split('\n')

    for line in fileList:
        if startTrigger in line:
            triggered = True
        if triggered:
            matchObj = re.match('\.text:[\dA-Z]+\s*(?:[A-F0-9]{2}\s)*\s*;?(.*);?', line)
            endOfSentMatch = re.match('\.text:[\dA-Z]+\s*; -+', line)
            endOfSentMatch2 = re.match('\.text:[\dA-Z].*loc_.*:.\s*; CODE XREF.*', line)
            stillInSectionMatch = re.match('\.text:.*', line)
            endOfSubroutineMatch = re.match('\.text:[\dA-Z]+\s*; [= SUBROUTINE]*', line)
            ddLineMatch = re.match('\.text:[\dA-Z]+\s*(?:[A-F0-9]{2}\s)*\s*.*dd\s(?:[A-F0-9]*h[,|\s]*)*.*', line)

            if not stillInSectionMatch:
                continue
            if endOfSubroutineMatch or endOfSentMatch or endOfSentMatch2:
                if '_text' in sentenceTokens:
                    sentenceTokens.remove('_text')
                if sentenceTokens:
                    listOfSentences.append(sentenceTokens)
                sentenceTokens = []
                continue
            if matchObj and not ddLineMatch:
                words = matchObj.groups(1)[0].split()
                for word in words:
                    bracketMatch = re.match('\[(\w{3}).*\]', word)
                    dWordMatch = re.match('dword\_.*', word)
                    subMatch = re.match('sub\_.*', word)
                    locMatch = re.match('loc\_.*', word)
                    offMatch = re.match('off\_.*', word)
                    byteMatch = re.match('byte\_.*', word)
                    unkMatch = re.match('unk\_.*', word)
                    locRetMatch = re.match('locret\_.*', word)
                    wordMatch = re.match('word\_.*', word )
                    dupMatch = re.match('dup\(.*\)', word)
                    if dupMatch:
                        word = "dup"
                    elif wordMatch:
                        word = "word_"
                    elif locRetMatch:
                        word = "locret_"
                    elif unkMatch:
                        word = "unk_"
                    elif byteMatch:
                        word = "byte_"
                    elif dWordMatch:
                        word = "dword_"
                    elif subMatch:
                        word = "sub_"
                    elif locMatch:
                        word = "loc_"
                    elif offMatch:
                        word = "off_"
                    elif bracketMatch:
                        word = '[' + bracketMatch.groups(1)[0] + ']'
                    sentenceTokens.extend(word)
    return listOfSentences


def extractID(fullPath):
    fileNameMatch = re.match('.*://?.*/(\w+).asm', fullPath)
    if fileNameMatch:
        return fileNameMatch.groups(1)[0]
    else:
        return "ERROR"

def vectorSubstitute(sentenceList, model):
    aggregatesList = []
    outputList = []
    nSentences = 0
    for sentence in sentenceList:
        baseV = np.zeros(100)
        nWords = 0.0
        for word in sentence:
            try:
                baseV = baseV + model[word]
                nWords += 1.0
            except KeyError:
                continue
        if nWords > 0.0:
            meanV = baseV/nWords
            outputList.append(meanV)
            nSentences += 1
    try:
        mean = np.mean(outputList, axis=0)
        var = np.var(outputList, axis=0)
        aggregatesList.extend(mean)
        aggregatesList.extend(var)
        for i in range(3):
            index = int(math.floor(random.random()*(nSentences-i)))
            aggregatesList.extend(outputList[index])
        return aggregatesList
    except TypeError:
        return np.zeros(500)




if __name__ == "__main__":
    sc = SparkContext()
    # for l in ['I', 'J', 'K', 'k', 'L', 'l', 'M', 'm', 'N', 'n', 'O', 'o', 'P', 'p', 'Q','q', 'R', 'r', 'S', 's', 'T', 't', 'U', 'u', 'V', 'v', 'W', 'w', 'X', 'x', 'Y', 'y', 'Z', 'z']:
    #     inFile = "s3n://AKIAI6V7LPMOP2MMTH4A:SWiXLmWIi06IjODaR45wciAV4t+uFU8tE3j8kQOh@malwaretestdata/test/" + l + "*.asm"
    #     outFile = "s3n://malwaredata/w2v/2p0/stemmedW2V_Test_" + l + str(round(random.random()*100))
    #     #inFile = "file:///Users/codywild/Desktop/MSAN/Module3/AdvancedML/KaggleMicrosoftMalware/dataSample/*.asm"
    #     #w2vFile = "/Users/codywild/Desktop/MSAN/Module3/AdvancedML/KaggleMicrosoftMalware/textW2V"
    #     w2vFile = "/root/w2v/stemmedW2V_120"
    #     w2v = sc.broadcast(Word2Vec.load(w2vFile))
    #
    #     sc.wholeTextFiles(inFile, 100)\
    #         .map(lambda x: (extractID(x[0]), sentenceParse(x[1], type="string")))\
    #         .map(lambda x: (x[0], vectorSubstitute(x[1], w2v.value)))\
    #         .saveAsTextFile(outFile)

    #for l in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E','F', 'G', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'H', 'I', 'J', 'K', 'L']:
    w2vFile = "/root/w2v/stemmedW2V_120"
    w2v = sc.broadcast(Word2Vec.load(w2vFile))
    for l in ['B', 'C', 'D', 'E','F', 'G', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'H', 'I', 'J', 'K', 'L']:

        inFile = "s3n://malwaredata/train/" + l + "*.asm"
        outFile = "s3n://malwaredata/w2v/2p0/condensed/condensedRemainder/stemmedW2V_Train_" + l + str(round(random.random()*100))

        sc.wholeTextFiles(inFile, 100)\
            .map(lambda x: (extractID(x[0]), sentenceParse(x[1], type="string")))\
            .map(lambda x: (x[0], vectorSubstitute(x[1], w2v.value)))\
            .saveAsTextFile(outFile)