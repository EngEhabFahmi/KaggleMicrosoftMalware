__author__ = "Cody Wild"

"""
This Spark parser was used on the full dataset to pull a sample of 'sentences' - i.e. internally coherent units from within the _idata code section 
of the .asm file. This 50% sample - stored in the form of a list of lists of tokens - was then used to train a Word2Vec model specific to the _data section
"""

import re
from pyspark import SparkContext
import random

def sentenceParse(fileStr, type="tup"):
    startTrigger = "_idata"
    triggered = False
    sentenceTokens = []
    listOfSentences = []

    if type == "tup":
        fileList = fileStr[1].split('\n')
    elif type == "list":
        fileList = fileStr
    elif type == "string":
        fileList = fileStr.split('\n')
    for line in fileList:
        if startTrigger in line:
            triggered = True
            continue
        if triggered:
            textDisqualify = re.match('\.idata:[0-9A-F]*\s*; \.text:[0-9A-F]*.*', line)
            xrefDisqualify = re.match('\.idata:[0-9A-F]*\s*;.* XREF', line)
            beginSentenceMatch = re.match('\.idata:[0-9A-F]*\s*;(.*)', line)
            endSentenceMatch = re.match('\.idata:[0-9A-F]*\s*(?:\?\?\s)+\s*(.*);.*', line)
            middleSentenceMatch = re.match('\.idata:[0-9A-F]*\s*(?:\?\?\s)+\s*(.*)(?!;)', line)
            stillInSectionMatch = re.match('\.idata:.*', line)

            if not stillInSectionMatch:
                continue
            if endSentenceMatch or textDisqualify or xrefDisqualify:
                if endSentenceMatch:
                    sentenceTokens.extend(endSentenceMatch.groups(1)[0].split())

                if '_idata' in sentenceTokens:
                    sentenceTokens.remove('_idata')
                if sentenceTokens:
                    listOfSentences.append(sentenceTokens)
                sentenceTokens = []
                continue
            if beginSentenceMatch:
                sentenceTokens.extend(beginSentenceMatch.groups(1)[0].split())
                continue
            if middleSentenceMatch:
                sentenceTokens.extend(middleSentenceMatch.groups(1)[0].split())
                continue


    return listOfSentences

if __name__ == "__main__":
    sc = SparkContext()
    sc.wholeTextFiles("s3n://malwaredata/train/*.asm", 4000)\
        .sample(False, 0.5)\
        .flatMap(sentenceParse)\
        .coalesce(100).saveAsTextFile("s3n://malwaredata/w2v/idata/fiftyPerc/idataSentences" + str(round(random.random(), 2)))



