__author__ = "Cody Wild"

"""
This code takes a significance mapping created by grouping the trained data, and create an aggregated vector version of that mapping 
to represent each document. It currently does this by aggregating (i.e. averaging) significance vectors on the sentence level and then returning two 
vectors: one for the mean of all the sentence vectors, and one for the variance of all the sentence vectors
"""
import numpy as np
def sigVecCondense(row, sigDF):
    output = []
    vecList = []
    nTokens = 0
    for key, val in row.iteritems():
        if key == 'fileName':
            ## add filename as first element in output
            output.append(val)
            continue
        elif not np.isnan(val):
            #print key
            if key in sigDF:
                for i in range(int(val)):
                    vecList.append(sigDF[key].values)
                nTokens += val
    vecList = np.asarray(vecList)
    output.append(nTokens)
    print nTokens
    sumVec = np.nansum(vecList, axis=0)
    meanVec = sumVec/nTokens
    varVec = np.nanmean(vecList, axis=0)
    output = output + list(meanVec)
    output = output + list(varVec)
    return output

if __name__ == "__main__":
    sigDF = pd.read_csv('/Users/codywild/Desktop/MSAN/Module3/AdvancedML/KaggleMicrosoftMalware/codeGen2/asm_parsing/simplified/significanceVectors.csv')

    for dSet in ['test', 'train']:
        print dSet
        runningDF = pd.DataFrame()
        for j in range(0, 21):
            print j
            data = pd.read_csv('/Users/codywild/Desktop/MSAN/Module3/AdvancedML/KaggleMicrosoftMalware/data/simplified/' + dSet + 'SimpleASM-CSV/' + dSet +'SimpleASM_' + str(j).zfill(2) + '.csv')
            condensedData = data.apply(lambda x: sigVecCondense(x, sigDF), axis=1)
            condensedData.to_csv(str(j) + "_sig.csv")
            runningDF = pd.concat([runningDF, condensedData])
        runningDF.to_csv('/Users/codywild/Desktop/MSAN/Module3/AdvancedML/KaggleMicrosoftMalware/data/simplified/' + dSet + "significance.csv")
        #
