__author__ = "Cody Wild"

"""
Using a pre-trained _idata Word2Vec model stored in text file form (essentially a dictionary mapping tokens to semantic vectors), this code hits the full dataset
and create an aggregated feature out of these semantic vectors by averaging across sentences, and then returning the mean and variance 
of the sentence vectors. 
"""

import re
import numpy as np
from pyspark import SparkContext
from gensim.models.word2vec import Word2Vec
import math
import random

def sentenceParse(fileStr, type="tup"):
    startTrigger = "_idata"
    triggered = False
    sentenceTokens = []
    listOfSentences = []
    i = 0

    if type == "tup":
        fileList = fileStr[1].split('\n')
    elif type == "list":
        fileList = fileStr
    elif type == "string":
        fileList = fileStr.split('\n')

    for line in fileList:
        if startTrigger in line:
            triggered = True
            continue
        if triggered:
            textDisqualify = re.match('\.idata:[0-9A-F]*\s*; \.text:[0-9A-F]*.*', line)
            xrefDisqualify = re.match('\.idata:[0-9A-F]*\s*;.* XREF', line)
            beginSentenceMatch = re.match('\.idata:[0-9A-F]*\s*;(.*)', line)
            endSentenceMatch = re.match('\.idata:[0-9A-F]*\s*(?:\?\?\s)+\s*(.*);.*', line)
            middleSentenceMatch = re.match('\.idata:[0-9A-F]*\s*(?:\?\?\s)+\s*(.*)(?!;)', line)
            stillInSectionMatch = re.match('\.idata:.*', line)

            if not stillInSectionMatch:
                continue
            if endSentenceMatch or textDisqualify or xrefDisqualify:
                if endSentenceMatch:
                    sentenceTokens.extend(endSentenceMatch.groups(1)[0].split())

                if '_idata' in sentenceTokens:
                    sentenceTokens.remove('_idata')
                if sentenceTokens:
                    listOfSentences.append(sentenceTokens)
                sentenceTokens = []
                continue
            if beginSentenceMatch:
                sentenceTokens.extend(beginSentenceMatch.groups(1)[0].split())
                continue
            if middleSentenceMatch:
                sentenceTokens.extend(middleSentenceMatch.groups(1)[0].split())
                continue


    return listOfSentences

def extractID(fullPath):
    fileNameMatch = re.match('.*://?.*/(\w+).asm', fullPath)
    if fileNameMatch:
        return fileNameMatch.groups(1)[0]
    else:
        return "ERROR"

def vectorSubstitute(sentenceList, model):
    aggregatesList = []
    outputList = []
    nSentences = 0
    for sentence in sentenceList:
        baseV = np.zeros(100)
        nWords = 0.0
        for word in sentence:
            try:
                baseV = baseV + model[word]
                nWords += 1.0
            except KeyError:
                continue
        if nWords > 0.0:
            meanV = baseV/nWords
            outputList.append(meanV)
            nSentences += 1
    try:
        mean = np.mean(outputList, axis=0)
        var = np.var(outputList, axis=0)
        aggregatesList.extend(mean)
        aggregatesList.extend(var)
        for i in range(3):
            index = int(math.floor(random.random()*(nSentences-i)))
            aggregatesList.extend(outputList[index])
        return aggregatesList
    except TypeError:
        return np.zeros(500)




if __name__ == "__main__":
    sc = SparkContext()
    for l in ['I', 'J', 'K', 'k', 'L', 'l', 'M', 'm', 'N', 'n', 'O', 'o', 'P', 'p', 'Q','q', 'R', 'r', 'S', 's', 'T', 't', 'U', 'u', 'V', 'v', 'W', 'w', 'X', 'x', 'Y', 'y', 'Z', 'z']:
        inFile = "s3n://AKIAI6V7LPMOP2MMTH4A:SWiXLmWIi06IjODaR45wciAV4t+uFU8tE3j8kQOh@malwaretestdata/test/" + l + "*.asm"
        outFile = "s3n://malwaredata/w2v/idata/vectorizedTest/stemmedW2V_Test_" + l + str(round(random.random()*100))
        #inFile = "file:///Users/codywild/Desktop/MSAN/Module3/AdvancedML/KaggleMicrosoftMalware/dataSample/*.asm"
        #w2vFile = "/Users/codywild/Desktop/MSAN/Module3/AdvancedML/KaggleMicrosoftMalware/textW2V"
        w2vFile = "/root/w2v/fiftyPercModel"
        w2v = sc.broadcast(Word2Vec.load(w2vFile))

        sc.wholeTextFiles(inFile, 100)\
            .map(lambda x: (extractID(x[0]), sentenceParse(x[1], type="string")))\
            .map(lambda x: (x[0], vectorSubstitute(x[1], w2v.value)))\
            .saveAsTextFile(outFile)

    for l in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E','F', 'G', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'H', 'I', 'J', 'K', 'L']:

        inFile = "s3n://malwaredata/train/" + l + "*.asm"
        outFile = "s3n://malwaredata/w2v/idata/vectorizedTrain/stemmedW2V_Train_" + l + str(round(random.random()*100))

        sc.wholeTextFiles(inFile, 100)\
            .map(lambda x: (extractID(x[0]), sentenceParse(x[1], type="string")))\
            .map(lambda x: (x[0], vectorSubstitute(x[1], w2v.value)))\
            .saveAsTextFile(outFile)