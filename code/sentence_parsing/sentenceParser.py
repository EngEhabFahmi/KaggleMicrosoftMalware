#sentenceParser2.py

import re
#from gensim.models.word2vec import Word2Vec
from pyspark import SparkContext
#from pyspark import SparkContext

def sentenceParse(fileStr, section='text', tup=True):
    if section == 'text':
        startTrigger = "_text"
    triggered = False
    sentenceTokens = []
    listOfSentences = []
    i = 0
    if tup:
        fileList = fileStr[1].split('\n')
    else:
        fileList = fileStr
    for line in fileList:
        if startTrigger in line:
            triggered = True
        if triggered:
            matchObj = re.match('\.text:[\dA-Z]+\s*(?:[A-F0-9]{2}\s)*\s*;?(.*);?', line)
            endOfSentMatch = re.match('\.text:[\dA-Z]+\s*; -+', line)
            endOfSentMatch2 = re.match('\.text:[\dA-Z].*loc_.*:.\s*; CODE XREF.*', line)
            stillInSectionMatch = re.match('\.text:.*', line)
            endOfSubroutineMatch = re.match('\.text:[\dA-Z]+\s*; [= SUBROUTINE]*', line)
            ddLineMatch = re.match('\.text:[\dA-Z]+\s*(?:[A-F0-9]{2}\s)*\s*.*dd\s(?:[A-F0-9]*h[,|\s]*)*.*', line)

            if not stillInSectionMatch:
                continue
            if endOfSubroutineMatch or endOfSentMatch or endOfSentMatch2:
                if '_text' in sentenceTokens:
                    sentenceTokens.remove('_text')
                if sentenceTokens:
                    listOfSentences.append(sentenceTokens)
                sentenceTokens = []
                continue
            if matchObj and not ddLineMatch:
                sentenceTokens.extend(matchObj.groups(1)[0].split())
    return listOfSentences

def stringType(s):
    if isinstance(s, str):
        return "ascii"
    elif isinstance(s, unicode):
        return "unicode"
    else:
        return "other"




if __name__ == "__main__":
    sc = SparkContext()
    #fileStr = f.readlines()
    # f.close()

    # sc.wholeTextFiles("s3n://malwaredata/train/*.asm", 1000)\
    #     .sample(False, 0.1)\
    #     .flatMap(sentenceParse).coalesce(100).saveAsTextFile("s3n://malwaredata/revisedSentenceSamplePoint1")

    sc.wholeTextFiles("file:///Users/codywild/Desktop/MSAN/Module3/AdvancedML/KaggleMicrosoftMalware/dataSample/*.asm", 4)\
        .flatMap(sentenceParse).saveAsTextFile("file:///Users/codywild/Desktop/MSAN/Module3/AdvancedML/KaggleMicrosoftMalware/dataSample/smallSample")
    # flattened = codeCorpus.flatMap(sentenceParse)
    # print flattened.take(1)
    #codeCorpus.saveAsTextFile("file:///Users/codywild/Desktop/MSAN/Module3/AdvancedML/KaggleMicrosoftMalware/dataSample/sentenceSample")

    #
    #
    # def gradient(model, sentences):  # executes on workers
    #     syn0, syn1 = model.syn0.copy(), model.syn1.copy()   # previous weights
    #     model.train(sentences)
    #     return {'syn0': model.syn0 - syn1, 'syn1': model.syn1 - syn1}
    #
    # def descent(model, update):      # executes on master
    #     model.syn0 += update['syn0']
    #     model.syn1 += update['syn1']
    #
    # def rddGenerator(rdd):
    #     yield rdd.take(1)[0]
    #
    # w2v = Word2Vec()
    # w2v.build_vocab(codeCorpus.collect())
    # w2v.train(codeCorpus.collect())
    # w2v.save("s3n://alwaredata/word2vecModel")
    # w2v.save('word2vecModel')

    # lOS = sentenceParse(fileStr)
    # fOut = open("sentences.txt", 'wb')
    # fOut.write(str(lOS))
    # fOut.close()

    #implement in a flatmap list of sentences
    #each observation is a sentence, i.e. list of words