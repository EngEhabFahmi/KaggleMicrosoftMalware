#!/usr/bin/env python
# -*- coding: utf-8 -*-
import re
from collections import *
import pandas as pd
from pyspark import SparkContext
import string

def asmReduce(a, b): 
    #(frequency_counter, section_length, num_subroutines, totalNumTokens)
    freqCounter = a[0] + b[0]
    meanDict = {}
    for key in a[1]: 
        aVal = a[key]
        bVal = b[key]
        meanDict[key] = (aVal+bVal)/2.0
    avgNumSubroutines = (a[3]+b[3])/2.0
    totalNumTokens = a[4] + b[4]
    return freqCounter, meanDict, avgNumSubroutines, totalNumTokens


def parseFile(fileName, prefixes=False):
    cnt = Counter()
    sectionsSeen = {}
    numSubroutines = 0
    numTokens = 0
    f = fileName.split('\n')
    for line in f:
        if ".data:00" in line:
            startTrigger = "_data"
            if prefixes:
                prefix = "DT_"
            else:
                prefix = ""
            if "_data" not in sectionsSeen:
                triggered = False

        elif ".rdata:00" in line:
            startTrigger = "_rdata"
            if prefixes:
                prefix = "RDT_"
            else:
                prefix = ""

            if "_rdata" not in sectionsSeen:
                triggered = False

        elif ".idata:00" in line:
            if prefixes:
                prefix = "IDT_"
            else:
                prefix = ""
            startTrigger = "_idata"
            if "_idata" not in sectionsSeen:
                triggered = False
        else:
            startTrigger = "_text"
            prefix = ""
            if "_text" not in sectionsSeen:
                triggered = False

        if startTrigger in line:
            triggered = True
            if startTrigger not in sectionsSeen:
                sectionsSeen[startTrigger] = 0

        if triggered:
            sectionsSeen[startTrigger] += 1
            if "S U B R O U T I N E" in line:
                numSubroutines += 1
                continue
            else:
                lineSplit = line.split('       ')
                if len(lineSplit) > 1:
                    assembCode = lineSplit[1]
                else:
                    assembCode = lineSplit[0]
                tokens = assembCode.split()
                for token in tokens:
                    if hasNoStopwords(token):
                        token = re.sub(r'[,;]','',token)
                        numTokens += 1 
                        if 'rdata' in token:
                            cnt['rdata'] += 1
                        elif 'loc' in token:
                            cnt['loc'] += 1
                        else:
                            cnt[prefix + token] += 1

    return cnt, sectionsSeen, numSubroutines, numTokens

def hasNoStopwords(token):
    stopwords = [".text:", '_text', 'Ãƒ', 'idata:', '_idata', 'rdata:', '_rdata', 'data:', '_data']
    for stopword in stopwords:
        if stopword in token:
            return False
    return True
def fileID(fileStr):
    isMatch = re.match('.*\/([0-9a-zA-Z]*)\.(asm|bytes)', fileStr)
    if isMatch:
        return isMatch.groups()[0]
    else:
        return ''
if __name__ == "__main__":
    sc = SparkContext("spark://ec2-52-10-133-145.us-west-2.compute.amazonaws.com:7077", "ASM Parse Test")
    asmFiles = sc.wholeTextFiles("/Users/codywild/Desktop/MSAN/Module3/AdvancedML/KaggleMicrosoftMalware/dataSample/*.asm")
    trainLabels = pd.read_csv("/Users/codywild/Desktop/MSAN/Module3/AdvancedML/KaggleMicrosoftMalware/trainLabels.csv")
    trainDict = trainLabels.set_iexport AWS_ACCESS_KEY_ID=AKIAJBLPYYHFYB7GJ33Andex('Id').to_dict()['Class']
    labelsBC = sc.broadcast(trainDict)
    asmMap = asmFiles.map(lambda x: (labelsBC.value[fileID(x[0])], parseFile(x[1].encode('utf-8'))))
    asmReduced = asmMap.reduceByKey(asmReduce)
    asmReduced.saveAsPickleFile('asmClassAgg', 1)

