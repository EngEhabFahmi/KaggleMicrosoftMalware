__author__ = "Cody Wild"

"""
Personal reference package of ML functions used by rForest
"""




from collections import Counter
import sklearn
import random
from math import *
import numpy as np
from sklearn.svm import SVC, LinearSVC
from sklearn.preprocessing import scale
from sklearn.cross_validation import KFold
from sklearn.utils import shuffle
import matplotlib.pyplot as plt



class zeroR(sklearn.base.BaseEstimator):
    """
    zeroR is a very simple classifier: for
    every test instance presented to it, the classifier returns
    the label that was seen most often in the training data.
    """

    def __init__(self):
        # if we haven't been trained, assume most frequent class is 1
        self.guess = 1
        self.type = "mostfrequent"

    def fit(self, data, labels):
        """
        Inputs: data: a list of X vectors
        labels: Y, a list of target values

        Find the most common label in the training data, and store it in self.guess
        Not required to return a value.
        """
        counts = Counter()
        for i in labels:
            counts[i] += 1
        self.guess = counts.most_common()[0][0]

    def predict(self, testData):
        """
        Input: testData: a list of X vectors to label.

        Classify all test data as the most common label.
        returns: a list of labels of the same length as testData, where each entry corresponds
        to the model's output on each example.
        """
        # TODO: Your code here
        return [self.guess for i in testData]


class chooseFeature(sklearn.base.BaseEstimator):
    """
    This defines a classifier that predicts on the basis of
      the feature that was found to have the best weighted purity, based on splitting all
      features according to their mean value. Then, for that feature split, it predicts
      a new example based on the mean value of the chosen feature, and the majority class for
      that split.
      You can of course define more variables!
    """

    def __init__(self):
        # if we haven't been trained, always return 1
        self.classForGreater= 1
        self.classForLeq = 1
        self.chosenFeature = 0
        self.featureMean = 0
        self.type = "chooseFeatureClf"

    def impurity(self, labels):
        #entropy formula:
        #- sum of P(value I) log2P(value I)

        #create counter, dictionary of labels and counts
        labelCounts = Counter(labels)
        entropy = 0
        n = float(len(labels))
        for labelValue in labelCounts:
            #calculate probability of a given label, dividing count by total N
            Pvi = labelCounts[labelValue]/n
            logPvi = log(Pvi, 2)
            sumterm = Pvi*logPvi
            entropy -= sumterm
        return entropy


    def weighted_impurity(self, list_of_label_lists):
        weighted_imp = 0
        for lst in list_of_label_lists:
            n = len(lst)
            imp = self.impurity(lst)
            #weighting: length of split times impurity of split
            weighted_imp += n*imp
        return weighted_imp


    def ftr_seln(self, data, labels):
        """return: index of feature with best weighted_impurity, when split
        according to its mean value; you are permitted to return other values as well,
        as long as the the first value is the index
        """
        #initialize arrays for impurities, means, and labels, to capture information
        #will be used by the fit function
        featureImpurities = []
        featureMeans = []
        splitLabels = []
        #loop through all features, identified by index
        for featureIndex in range(len(data[0])):
            featureCol = [lst[featureIndex] for lst in data]
            featureMean = np.mean(featureCol)
            featureMeans.append(featureMean)
            labelsGTMean = []
            labelsLTMean = []
            for j in range(len(featureCol)):
                if featureCol[j] <= featureMean:
                    labelsLTMean.append(labels[j])
                else:
                    labelsGTMean.append(labels[j])
            splitLabels.append([labelsGTMean, labelsLTMean])
            featureImpurity = self.weighted_impurity([labelsGTMean, labelsLTMean])
            featureImpurities.append(featureImpurity)
        featureImpurities = np.asarray(featureImpurities)
        bestI = np.where(featureImpurities == featureImpurities.min())
        bestI = bestI[0][0]
        #print "Best index is: " + str(bestI)
        mean = featureMeans[bestI]
        splits = splitLabels[bestI]

        #returns the best index, as well as that features mean, and the splits generated by that mean
        return bestI, mean, splits



    def fit(self, data, labels):
        """
        Inputs: data: a list of X vectors
        labels: Y, a list of target values
        """
        chosenFeature, featureMean, splitLabels  = self.ftr_seln(data, labels)
        gteLabels = splitLabels[0]
        ltLabels = splitLabels[1]

        gtModeLabel = Counter(gteLabels).most_common(1)[0][0]
        lteModeLabel = Counter(ltLabels).most_common(1)[0][0]


        self.classForGreater= gtModeLabel
        self.classForLeq = lteModeLabel
        self.chosenFeature = chosenFeature
        self.featureMean = featureMean

    def predict(self, testData):
        """
        Input: testData: a list of X vectors to label.
        Check the chosen feature of each
        element of testData and make a classification decision based on it
        """
        cf = self.chosenFeature
        returnVals = []
        for data in testData:
            cfVal = data[cf]
            if cfVal <= self.featureMean:
                returnVals.append(self.classForLeq)
            else:
                returnVals.append(self.classForGreater)
        return returnVals

def transform_sklearn_dictionary(input_dict):
    """ Input: input_dict: a Python dictionary or dictionary-like object containing
    at least information to populate a labeled dataset, L={X,y}
    return:
    X: a list of lists. The length of inner lists should be the number of features,
     and the length of the outer list should be the number of examples.
    y: a list of target variables, whose length is the number of examples.
    X & y are not required to be numpy arrays, but you may find it convenient to make them so.
    """
    # TODO: Your code here
    X = input_dict["data"]
    y = np.ravel(np.asarray(input_dict["target"]))
    return X, y



def transform_csv(data, target_col=0, ignore_cols=None):
    """ Input: data: a pandas DataFrame
    return: a Python dictionary with same keys as those used in sklearn's iris dataset
    (you don't have to create an object of the same data type as those in sklearn's datasets)
    """
    my_dictionary = {}
    my_dictionary["target"] = data.ix[:,[target_col]]

    #check for numeric index on target
    try:
        target_col = float(target_col)
        xdata = data.drop(data.columns[target_col], axis=1)
    except ValueError:
        xdata = data.drop(target_col, axis=1)

    #check for numeric index on ignore columns
    try:
        ig = ignore_cols[0]
        try:
            float(ig)
            xdata = xdata.drop(data.columns[ignore_cols], axis=1)
        except ValueError:
            xdata = xdata.drop(ignore_cols, axis=1)
    except TypeError:
        pass


    my_dictionary["data"] = [list(x) for x in xdata.itertuples()]
    my_dictionary["feature_names"] = xdata.columns.values
    my_dictionary["target_names"] = "target"
    my_dictionary["DESCR"] = "Hi there!"
    return my_dictionary

def plot(pTitle, mean_test_score_lin, std_test_score_lin, mean_test_scoreRBF, std_test_scoreRBF):
    cRange = [0.1, 0.5, 1, 2, 4, 16, 64]
    plt.title(pTitle)
    plt.xlabel("C")
    plt.ylabel("Score")
    plt.ylim(0.5, 0.9)

    plt.semilogx(cRange, mean_test_score_lin, label="Linear", color="r")

    plt.fill_between(cRange, mean_test_score_lin - std_test_score_lin,
                     mean_test_score_lin + std_test_score_lin, alpha=0.2, color="r")

    plt.semilogx(cRange, mean_test_scoreRBF, label="RBF",
                 color="b")
    plt.fill_between(cRange, mean_test_scoreRBF - std_test_scoreRBF,
                     mean_test_scoreRBF + std_test_scoreRBF, alpha=0.2, color="b")
    plt.legend(loc="best")
    plt.savefig("".join(pTitle.split()) + ".png")
    plt.close()

def resampleIndexes(dataLen):
    """
    Create a set of bootstrap indexes of the data of the same size.
    :param data: length
    :return: Tuple: (sample indexes, OOB indexes)
    """
    dataInd = []
    for i in range(dataLen):
        rand = random.randrange(dataLen)
        dataInd.append(rand)
    #test data, if needed
    unsampled = list(set(range(dataLen)).difference(dataInd))
    return dataInd, unsampled

def weightedChoice_Indexes(dataLen, weights):
    """
    Based on: http://stackoverflow.com/questions/10803135/weighted-choice-short-and-simple
    :param data: list of any format you want! Usually a dataset for ML
    :param weights: weight vector of same length as data
    :return: Sampled version of data, of same length, with a given sample likely to occur with
    probability associated with its weight.
    """
    cs = np.cumsum(weights)
    result_indexes = []
    for x in range(dataLen):
        #find the index of the first weight over a random value
        idx = sum(cs < np.random.rand())
        result_indexes.append(idx)
    return result_indexes


def nFoldKTimesPlot(nf, k, X, y):
    X = scale(np.asarray(X).astype(np.float))
    y = np.asarray(y)
    cRange = [0.1, 0.5, 1, 2, 4, 16, 64]
    mean_test_scoreRBF = np.empty(7)
    mean_test_score_lin = np.empty(7)
    std_test_scoreRBF = np.empty(7)
    std_test_score_lin = np.empty(7)


    for i in range(len(cRange)):
        rbfScoresThisC = []
        linScoresThisC = []
        c = cRange[i]
        print c
        for iteration in range(k):
            rbfScoresThisIter = []
            linScoresThisIter = []
            for train_index, test_index in KFold(1000, n_folds=nf, shuffle=True):
                X_train, X_test = X[train_index], X[test_index]
                y_train, y_test = y[train_index], y[test_index]
                rbf = SVC(C=c)
                lin = LinearSVC(C=c)
                rbfFitted = rbf.fit(X_train, y_train)
                rbfScoresThisIter.append(rbfFitted.score(X_test, y_test))

                linFitted = lin.fit(X_train, y_train)
                linScoresThisIter.append(linFitted.score(X_test, y_test))
            rbfScoresThisC.append(np.mean(rbfScoresThisIter))
            linScoresThisC.append(np.mean(linScoresThisIter))

        mean_test_scoreRBF[i] = (round(np.mean(rbfScoresThisC), 3))
        mean_test_score_lin[i] = (round(np.mean(linScoresThisC), 3))
        std_test_scoreRBF[i] = (round(np.std(rbfScoresThisC), 3))
        std_test_score_lin[i] = (round(np.std(linScoresThisC), 3))
    print cRange
    print mean_test_scoreRBF
    print mean_test_score_lin
    print std_test_scoreRBF
    print std_test_score_lin
    plot(str(nf) + "-Fold " + str(k) + " Times Validation Curve", mean_test_score_lin, std_test_score_lin, mean_test_scoreRBF, std_test_scoreRBF)

def nFoldPlot(nf, X, y):
    X = scale(np.asarray(X).astype(np.float))
    y = np.asarray(y)
    cRange = [0.1, 0.5, 1, 2, 4, 16, 64]
    mean_test_scoreRBF = np.empty(7)
    mean_test_score_lin = np.empty(7)
    std_test_scoreRBF = np.empty(7)
    std_test_score_lin = np.empty(7)


    for i in range(len(cRange)):
        rbfScores = []
        linScores = []
        c = cRange[i]
        print c
        j = 0
        for train_index, test_index in KFold(1000, n_folds=nf, shuffle=True):
            print j
            X_train, X_test = X[train_index], X[test_index]
            y_train, y_test = y[train_index], y[test_index]
            rbf = SVC(C=c)
            lin = LinearSVC(C=c)
            rbfFitted = rbf.fit(X_train, y_train)
            rbfScores.append(rbfFitted.score(X_test, y_test))

            linFitted = lin.fit(X_train, y_train)
            linScores.append(linFitted.score(X_test, y_test))
            j += 1
        mean_test_scoreRBF[i] = (np.mean(rbfScores))
        mean_test_score_lin[i] = (np.mean(linScores))
        std_test_scoreRBF[i] = (np.std(rbfScores))
        std_test_score_lin[i] = (np.std(linScores))
    print cRange
    print mean_test_scoreRBF
    print mean_test_score_lin
    print std_test_scoreRBF
    print std_test_score_lin
    plot(str(nf) + " Fold Twice Validation Curve", mean_test_score_lin, std_test_score_lin, mean_test_scoreRBF, std_test_scoreRBF)


def randomNSplitsPlot(nf, X, y):
    X = scale(np.asarray(X).astype(np.float))
    y = np.asarray(y)
    cRange = [0.1, 0.5, 1, 2, 4, 16, 64]
    mean_test_scoreRBF = np.empty(7)
    mean_test_score_lin = np.empty(7)
    std_test_scoreRBF = np.empty(7)
    std_test_score_lin = np.empty(7)


    for i in range(len(cRange)):
        rbfScores = []
        linScores = []
        c = cRange[i]
        print c
        for k in range(nf):
            XSh, ySh = shuffle(X, y)
            print XSh[0]
            X_train, X_test = XSh[0:900], XSh[901:1000]
            y_train, y_test = ySh[0:900], ySh[901:1000]
            rbf = SVC(C=c)
            lin = LinearSVC(C=c)
            rbfFitted = rbf.fit(X_train, y_train)
            rbfScores.append(rbfFitted.score(X_test, y_test))

            linFitted = lin.fit(X_train, y_train)
            linScores.append(linFitted.score(X_test, y_test))

        mean_test_scoreRBF[i] = (np.mean(rbfScores))
        mean_test_score_lin[i] = (np.mean(linScores))
        std_test_scoreRBF[i] = (np.std(rbfScores))
        std_test_score_lin[i] = (np.std(linScores))
    print cRange
    print mean_test_scoreRBF
    print mean_test_score_lin
    print std_test_scoreRBF
    print std_test_score_lin
    plot(str(nf) + " Random Splits Validation Curve", mean_test_score_lin, std_test_score_lin, mean_test_scoreRBF, std_test_scoreRBF)




def bootstrapFoldsPlot(nf, X, y):
    X = scale(np.asarray(X).astype(np.float))
    y = np.asarray(y)
    cRange = [0.1, 0.5, 1, 2, 4, 16, 64]
    mean_test_scoreRBF = np.empty(7)
    mean_test_score_lin = np.empty(7)
    std_test_scoreRBF = np.empty(7)
    std_test_score_lin = np.empty(7)


    for i in range(len(cRange)):
        rbfScores = []
        linScores = []
        c = cRange[i]
        print c
        for k in range(nf):
            sample_indexes, outOfBag_indexes = resampleIndexes(1000)
            X_train, X_test = X[sample_indexes], X[outOfBag_indexes]
            y_train, y_test = y[sample_indexes], y[outOfBag_indexes]
            rbf = SVC(C=c)
            lin = LinearSVC(C=c)
            rbfFitted = rbf.fit(X_train, y_train)
            rbfScores.append(rbfFitted.score(X_test, y_test))

            linFitted = lin.fit(X_train, y_train)
            linScores.append(linFitted.score(X_test, y_test))

        mean_test_scoreRBF[i] = (np.mean(rbfScores))
        mean_test_score_lin[i] = (np.mean(linScores))
        std_test_scoreRBF[i] = (np.std(rbfScores))
        std_test_score_lin[i] = (np.std(linScores))
    print cRange
    print mean_test_scoreRBF
    print mean_test_score_lin
    print std_test_scoreRBF
    print std_test_score_lin
    plot(str(nf) + " Bootstrapped Splits Validation Curve", mean_test_score_lin, std_test_score_lin, mean_test_scoreRBF, std_test_scoreRBF)
