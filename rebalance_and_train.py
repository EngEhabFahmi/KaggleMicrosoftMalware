__author__ = 'David Reilly'

import urllib2
import csv
import pandas as pd
import numpy as np
from sklearn import preprocessing
from sklearn import cross_validation
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import log_loss
import random
from joblib import Parallel, delayed
import time

"""
    This script was created for the primary purpose of determing whether
    or not training on a rebalanced data set would positively affect the
    test set log loss. Passing a valid path to main() uses all available
    cores to determine the best set of proportions and writes the best 100
    to a text file 'resampling.txt'
"""

## This function takes a path to a csv and returns a pandas data frame
def url_to_df(path):

    ## Read in string
    url = urllib2.urlopen(path)
    text = url.read()

    ## Transform string into csv
    reader = csv.reader(text.split('\n'), delimiter=',')
    rows = [add_zeros_list(row) for row in reader]

    ## Collect column names
    colnames = rows[0]
    colnames[-260:] = [str(x) + "_" if x is float else x for x in colnames[-260:]]
    rows.pop(0)

    ## Create data frames, drop erroneous columns
    df = pd.DataFrame(rows, columns=colnames).fillna(0)
    df.drop(0.0, axis=1, inplace=True)
    df.drop('Unnamed: 0', axis=1, inplace=True)

    return df

## This function turns a pandas data frame into two numpy vectors, X and y
def pandas_to_numpy(data_frame, target, features, scale=False):

    X, y = np.array(data_frame.ix[:, features]), np.array(data_frame.ix[:, target])

    ## Shuffle indices
    indices = np.arange(y.shape[0])
    np.random.shuffle(indices)
    X, y = X[indices], y[indices]

    ## If scale=True, scale X to mean zero and unit variance
    if scale:
        X = preprocessing.scale(X.astype(float))

    return X, y

## This generator function returns a subset of unique columns of a data frame
def subset_columns(data_frame, numb_splits):


    numb_columns = len(data_frame.columns.values)
    length_of_split = floor(numb_columns / float(numb_splits))

    start_position = 0
    while start_position <= len(data_frame.columns.values):
        df = data_frame.ix[:, start_position:(start_position + length_of_split)]
        yield np.array(df.T.drop_duplicates().T)
        start_position += length_of_split

## This function takes a list of paths to csv's and returns only the unique columns from each
def grab_train(list_of_paths):

    ## Create list of data frames and concatenate them together
    print "Generating data frames..."
    try:
        # data_frames = [url_to_df(j) for j in list_of_paths]
        print "Successfully generated data frames..."
        print "Concatenating data frames..."
        df = pd.concat(list(url_to_df(j) for j in list_of_paths), axis=1)
        print "Successfully Concatenated data frames..."
    except Exception as e:
        print e

    # final_df = df.ix[:, set(df.columns.values)]
    print "Removing duplicate columns..."
    print len(df.columns.values)
    print len(df)
    final_df = pd.DataFrame(np.concatenate(subset_columns(df, 200), axis=1))
    print "Removed duplicate columns..."
    print final_df.head()
    print len(final_df.columns.values)
    print len(final_df)

    return final_df

## Replace each empty string with 0
def add_zeros_list(list):

    for i, j in enumerate(list):
        if j == '':
            list[i] = 0.0
        else:
            try:
                list[i] = float(j)
            except ValueError:
                list[i] = j

    return list

## Create list of paths to csv's
def generate_paths():

    numbers = range(21)
    list_of_paths = []
    for i in numbers:
        if i <= 9:
            path = "/path/to/bytes/csv/{0}.csv".format('0' + str(i))
            list_of_paths.append(path)
        else:
            path = "/path/to/bytes/csv/{0}.csv".format(str(i))
            list_of_paths.append(path)

    return list_of_paths

## This function takes a list of proportions that sum to 1 and samples
## the indices of each class with that proportion
def rebalance(X, y, proportions):

    ## Generate indexes
    indexes = {}
    for classification in [1, 2, 3, 4, 5, 6, 7, 8, 9]:
        indexes['p_{0}_indexes'.format(classification)] = [j for j, item in enumerate(y) if item == classification]

    ## Sample indexes
    samples = {}
    for i, j in enumerate(proportions):
        samples['sample_{0}'.format(i)] = np.random.choice(indexes['p_{0}_indexes'.format(i + 1)], size=(len(y) * j),
                                                           replace=True)

    ## Concatenate all sample indexes together
    X = X[np.concatenate([value for key, value in samples.iteritems()])]
    y = y[np.concatenate([value for key, value in samples.iteritems()])]

    return X, y

def parallel_grid_search(X, y, iteration):

    ## Determine which proportions to use for resampling
    resample = True
    if iteration > 0:
        resample = False
        proportions = [None]
    else:
        ## Generate random set of proportions that sum to 1
        proportions = [random.randint(1, 9) for i in range(9)]
        total = float(np.sum(proportions))
        proportions = map(lambda x: x / total, proportions)

    ## Fit a random forest, generate training and test folds
    forest = RandomForestClassifier(n_estimators=100, n_jobs=-1)
    kf = cross_validation.StratifiedKFold(y, n_folds=5)

    ## For each fold, rebalance training fold and test on unbalanced data
    mean_log_loss = []
    for train_idx, test_idx in kf:
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]
        if resample:
            X_train, y_train = rebalance(X_train, y_train, proportions)
        forest.fit(X_train, y_train)
        predictions = forest.predict_proba(X_test)
        mean_log_loss.append(log_loss(y_test, predictions))

    return {"Proportions": proportions, "Log Loss": np.mean(mean_log_loss)}

## This function uses joblib in order to parallelize the stochastic resampling search
def grid_search(data_frame, target, features):

    ## Set seed, send jobs to all available cores
    np.random.seed(0)
    X, y = pandas_to_numpy(data_frame, target, features, scale=False)
    hyper_proportions = Parallel(n_jobs=-1)(delayed(parallel_grid_search)(X, y, j) for j in range(1500))

    ## Return sorted proportions by ascending log loss
    best_log_loss = sorted(hyper_proportions, key=lambda x: x["Log Loss"])

    return best_log_loss

## This function creates the pandas data frame and returns the sorted sampling proportions
def load_data_frames(path_train):

    df = pd.read_csv(path_train)
    df.fillna(0, inplace=True)
    target = 'class'
    features = [el for el in df.columns if el != target and el != 'file' and el != 'Unnamed: 0']

    return grid_search(df, target, features)

## Write best 100 log loss random forest fits to txt file
def main(path):
    list_of_iterations = load_data_frames(path)
    with open("resampling.txt", "wb") as resampling:
        resampling.write(str(list_of_iterations[:100]))

if __name__ == "__main__":
    start_time = time.time()
    path = "/path/to/data"
    # main(path)
    print time.time() - start_time